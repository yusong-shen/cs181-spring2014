{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf190
{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;\red255\green255\blue255;\red38\green38\blue38;\red47\green102\blue178;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl520

\f0\fs40 \cf0 \cb2 Machine Learning Books
\fs26 \cf3 \
\pard\pardeftab720\sl320\sa40
\cf3 By Prof. Ryan Adams, From Piazza Apr 1, 2014\
I've been receiving a lot of feedback about the Bishop book. \'a0I know a lot of you don't like it and I completely agree that it is imperfect. \'a0It's difficult to believe, I'm sure, but it really seems to be the best book on the market right now. \'a0Part of the issue is simply that machine learning is a new field. \'a0Whereas there are hundreds, if not thousands, of textbooks on calculus and introductory statistics, there simply hasn't been enough time for the field to settle down on a clear pedagogical approach. \'a0I've spent a lot of time looking at different options and I really think the Bishop book is the best one. \'a0In a recent email to one of you, I likened it to learning a foreign language. \'a0In the beginning, it's hard because the symbols are unfamiliar and you're sure the underlying ideas aren't that complicated. \'a0Over time, you begin to feel comfortable with the abstractions, but it's painful to get going.\
\'a0\
That said, we all learn in different ways, and I thought it might be helpful to identify some of the other books out there. \'a0This might give you some resources if you want to find an alternative, and might also help you understand why I think the Bishop book is the best one.\
\'a0\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl400
\ls1\ilvl0\cf3 {\listtext	\'95	}Hastie, Tibshirani & Friedman, {\field{\*\fldinst{HYPERLINK "http://statweb.stanford.edu/~tibs/ElemStatLearn/"}}{\fldrslt \cf4 Elements of Statistical Learning}}. ESL is probably the most common book for serious machine learning courses. \'a0It is rigorous and comprehensive, although it focuses heavily on a statistical view of supervised learning. \'a0Freely available online. It more more less assumes that you're already well trained in statistics and doesn't really have a CS view at all. \'a0It does not cover unsupervised learning very well and does not cover reinforcement learning. \'a0tl;dr - Way too hard and statistical for CS 181.\
{\listtext	\'95	}Murphy, {\field{\*\fldinst{HYPERLINK "http://www.cs.ubc.ca/~murphyk/MLbook/"}}{\fldrslt \cf4 Machine Learning: A Probabilistic Perspective}}. \'a0Kevin Murphy is a big reason I'm in this field (he was a postdoc at MIT when I was an undergrad). This book has great coverage and goes over more recent developments than Bishop. \'a0It is encyclopedic, but pays for that breadth in that the explanations are not as good and the figures are often low quality. \'a0Right now there are an embarrassing number of errors. \'a0I used it for my graduate course, CS 281, this past fall and was shocked at how many typos the book had. \'a0Covers everything we care about in CS 181 except for reinforcement learning. \'a0tl;dr - More notation, less explanation, too many mistakes for CS 181.\
{\listtext	\'95	}James, Witten, Hastie & Tibshirani, {\field{\*\fldinst{HYPERLINK "http://www-bcf.usc.edu/~gareth/ISL/"}}{\fldrslt \cf4 Introduction to Statistical Learning with Applications in R}}. This is a brand-new book and it looks like "Baby ESL". \'a0I gave this one a very close look. \'a0However, it's gone too far the wrong way. \'a0It's too lightweight and just teaches at a high level with no real details. \'a0It is about teaching you to use the R packages they supply and I don't view CS 181 as being about learning to use other people's tools. \'a0For example, it doesn't even talk about how you'd do linear regression with multiple inputs, because that would require talking about matrices and is considered out of scope. \'a0Suffers from the same limited coverage and supervised focus of ESL. \'a0tl;dr - Too lightweight for a serious ML class and who wants to use R?\
{\listtext	\'95	}Mitchell, {\field{\*\fldinst{HYPERLINK "http://www.cs.cmu.edu/~tom/mlbook.html"}}{\fldrslt \cf4 Machine Learning}}. This used to be a popular book, but it was published in 1997 and hasn't been updated. \'a0The field moves too fast and there have been too many important innovations in the last twenty years to pretend that a mid-nineties snapshot is appropriate for this topic. \'a0The topics of half of the chapters are woefully out of date. \'a0It has a reputation as being readable, but I don't think you can center a course around it anymore. tl;dr - Too old and out of date.\
{\listtext	\'95	}Barber, {\field{\*\fldinst{HYPERLINK "http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage"}}{\fldrslt \cf4 Bayesian Reasoning and Machine Learning}}. For what it covers, it looks great, but it's pretty narrow. \'a0Freely available online. I like being Bayesian as much as the next guy (maybe more!) but I can't pretend like that's the only way to tackle these problems. \'a0It's very heavy on graphical models and inference, which are certainly important but I don't think these are the right backbone on which to build a course. \'a0No SVMs, no reinforcement learning, no neural networks. \'a0tl;dr - Too narrowly focused on Bayesian graphical models.\
{\listtext	\'95	}Alpaydin, {\field{\*\fldinst{HYPERLINK "http://www.cmpe.boun.edu.tr/~ethem/i2ml2e/index.html"}}{\fldrslt \cf4 Introduction to Machine Learning}}. I also gave this one a very close look, because it appears to cover an ideal range of topics. I've never heard of the author, but that shouldn't be a deterrent. \'a0In the end, I didn't go with it because it was just too superficial in its treatment of important topics. \'a0It only spends a page or two on things that I think need a whole chapter. \'a0It's easier reading than Bishop but you don't learn all that much. \'a0tl;dr - Good coverage, but too shallow.\
{\listtext	\'95	}Rogers & Girolami, {\field{\*\fldinst{HYPERLINK "http://www.dcs.gla.ac.uk/~srogers/firstcourseml/"}}{\fldrslt \cf4 A First Course in Machine Learning}}. Mark Girolami is a friend and has an amazing mustache that has now grown into a truly impressive beard. \'a0There's a lot to like about this book. \'a0It's Bayesian but not too Bayesian. \'a0However, it goes too lightweight and doesn't cover a lot of things that I think need to appear in an introductory course: neural networks, SVMs, HAC. tl;dr - Good explanations, but limited topics covered. Author has great mustache.\
{\listtext	\'95	}Sutton & Barto, {\field{\*\fldinst{HYPERLINK "http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html"}}{\fldrslt \cf4 Reinforcement Learning: An Introduction}}. Absolute classic and the authoritative book on RL. \'a0However, only about RL. \'a0tl;dr - Reinforcement learning only.\
{\listtext	\'95	}Duda, Hart & Stork. {\field{\*\fldinst{HYPERLINK "http://www.amazon.com/Pattern-Classification-Pt-1-Richard-Duda/dp/0471056693"}}{\fldrslt \cf4 Pattern Classification}}. This book is also a classic, first published in 1973! \'a0It covers a lot of what we care about in CS 181 and although slightly stale it has aged much better than Mitchell in my opinion. \'a0It is currently my leading candidate for replacing Bishop next year. tl;dr - Might be the right one to look at if you hate Bishop.\
{\listtext	\'95	}Koller & Friedman. {\field{\*\fldinst{HYPERLINK "http://pgm.stanford.edu/"}}{\fldrslt \cf4 Probabilistic Graphical Models: Principles and Techniques}}. Everything you want to know about graphical models. \'a0Nothing about anything else we care about in a general machine learning course. tl;dr - Only about graphical models.\
{\listtext	\'95	}Russel & Norvig. {\field{\*\fldinst{HYPERLINK "http://aima.cs.berkeley.edu/"}}{\fldrslt \cf4 Artificial Intelligence - A Modern Approach}}. This is the classic "must own" book on artificial intelligence. \'a0If you took CS 182, it probably used this book. \'a0It's a great one to have on your shelf, but machine learning is just a small part of it and it is not comprehensive in any way. \'a0There are lots of important things it just doesn't talk about. \'a0tl;dr - Buy it, but not for the machine learning.\
{\listtext	\'95	}MacKay. {\field{\*\fldinst{HYPERLINK "http://www.inference.phy.cam.ac.uk/mackay/itila/"}}{\fldrslt \cf4 Information Theory, Inference, and Learning Algorithms}}. David MacKay was my PhD supervisor and I think this book is wonderful. \'a0Freely available online. David is a genius at explaining things and this comes through in the book. \'a0However, the book is unbelievably idiosyncratic and only a little bit of the book is actually about machine learning. \'a0It blends together lots of interesting things: physics, coding theory, neural networks, etc, but it would not provide an adequate introduction to many of the things you would want to learn in CS 181. \'a0No HAC, PCA, SVMs, RL, HMMs, etc. \'a0David's explanation of backpropagation is "This gradient can be efficiently computed using the backpropagation algorithm (Rumelhart et al., 1986), which uses the chain rule to find the derivatives." tl;dr - A wonderful catalog of things David MacKay is interested in, with beautiful explanations thereof.\
}